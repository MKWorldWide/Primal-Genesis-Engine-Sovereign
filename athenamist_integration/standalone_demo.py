#!/usr/bin/env python3
"""
AthenaMist-Blended Standalone Demo Application
==============================================

This module provides the main application interface for AthenaMist-Blended, offering
a comprehensive AI assistant experience with government contract data integration.

Key Features:
- Multi-mode AI personality system (Creative, Technical, Workflow, Government)
- Real AI provider integration (Mistral AI, OpenAI)
- SAM government contract data access and analysis
- Interactive command-line interface with natural language processing
- Comprehensive conversation history and context management
- Workflow suggestions and productivity optimization
- Performance monitoring and system health tracking

Architecture:
- Modular AI integration with provider switching
- Secure SAM API integration with encrypted key management
- Async/await pattern for high-performance operations
- Context-aware response generation
- Intelligent query routing and processing

User Experience:
- Intuitive command-line interface
- Natural language query processing
- Real-time AI responses with context awareness
- Comprehensive help system and command documentation
- Performance metrics and system status monitoring

Security Features:
- Secure API key management
- Input validation and sanitization
- Error handling without sensitive data exposure
- Audit logging and monitoring

Performance Optimizations:
- Async operations for non-blocking user experience
- Intelligent caching and response optimization
- Memory-efficient conversation history management
- Background task processing

Dependencies:
- asyncio: Async programming support
- aiohttp: Async HTTP client for API calls
- requests: HTTP library for fallback operations
- json: Data serialization and configuration
- time: Timestamp and performance tracking

Author: AthenaMist Development Team
Version: 1.0.0
Last Updated: 2024-12-19
"""

import asyncio
import json
import random
import time
import os
from typing import Dict, List, Optional
import requests
import aiohttp

# Import AI and SAM integration modules
from core.sam_integration import SAMIntegration, AthenaMistSAMIntegration
from core.ai_integration import AIIntegrationManager, configure_ai_provider

class AthenaMistAI:
    """
    AthenaMist AI Core - Standalone Version with Real AI Integration
    
    This class provides the core AI functionality for AthenaMist-Blended, integrating
    multiple AI providers with government contract data analysis capabilities.
    
    Features:
    - Multi-mode personality system with context-aware responses
    - Real AI provider integration with fallback mechanisms
    - SAM government contract data integration
    - Conversation history management and context building
    - Intelligent query routing and processing
    - Workflow suggestions and productivity optimization
    
    AI Capabilities:
    - Natural language understanding and processing
    - Context-aware response generation
    - Multi-modal personality adaptation
    - Intelligent query classification and routing
    - Performance monitoring and optimization
    
    Integration Features:
    - Seamless AI provider switching
    - SAM data access and analysis
    - Conversation persistence and context management
    - Error handling and recovery mechanisms
    - Performance metrics and health monitoring
    """
    
    def __init__(self, mode: str = "creative", ai_provider: str = "mistral", ai_api_key: str = None):
        """
        Initialize AthenaMist AI with comprehensive configuration
        
        This method sets up the AI system with personality modes, provider integration,
        and SAM data access capabilities.
        
        Args:
            mode (str): AI personality mode (creative/technical/workflow/government)
            ai_provider (str): AI provider preference (mistral/openai)
            ai_api_key (str): API key for AI provider
            
        Personality Modes:
        - creative: Artistic and imaginative responses
        - technical: Precise and analytical responses
        - workflow: Practical and efficiency-focused
        - government: SAM and contract-focused responses
        
        Initialization Features:
        - Personality configuration and setup
        - AI provider integration and validation
        - SAM integration with secure key management
        - Conversation history initialization
        - Performance monitoring setup
        """
        # Core configuration and personality setup
        self.mode = mode
        self.personality = self._get_personality(mode)
        self.conversation_history = []
        self.workflow_suggestions = []
        
        # Performance monitoring and metrics
        self.query_count = 0
        self.response_times = []
        self.error_count = 0
        self.start_time = time.time()
        
        # Initialize AI integration with provider management
        self.ai_manager = configure_ai_provider(ai_provider, ai_api_key)
        
        # Initialize SAM integration with secure API key management
        sam_api_key = "gkwM6H5pnxU2qEkPJLp4UT9OwBfuLLonsovaU2Im"
        self.sam_integration = SAMIntegration(sam_api_key)
        self.sam_ai = AthenaMistSAMIntegration(self.sam_integration)
        
        # System health and status tracking
        self.system_status = {
            'ai_provider': ai_provider,
            'mode': mode,
            'start_time': self.start_time,
            'last_query_time': None,
            'health_status': 'healthy'
        }
        
    def _get_personality(self, mode: str) -> Dict:
        """
        Get AI personality configuration based on selected mode
        
        This method provides comprehensive personality configurations for different
        AI modes, including greetings, styles, and focus areas.
        
        Args:
            mode (str): Personality mode to configure
            
        Returns:
            Dict: Personality configuration with name, style, focus, and greeting
            
        Personality Configurations:
        - creative: Inspiring and artistic personality for creative workflows
        - technical: Precise and analytical personality for technical tasks
        - workflow: Efficient and organized personality for productivity
        - government: Official and comprehensive personality for SAM data
        """
        personalities = {
            "creative": {
                "name": "AthenaMist Creative",
                "style": "inspiring and artistic",
                "focus": "creative workflows and artistic expression",
                "greeting": "ğŸŒŸ Greetings, creative soul! I am AthenaMist, your AI companion for artistic endeavors and government contract insights. What shall we create today?"
            },
            "technical": {
                "name": "AthenaMist Technical",
                "style": "precise and analytical",
                "focus": "technical workflows and optimization",
                "greeting": "âš™ï¸ Hello! I am AthenaMist Technical, your AI assistant for precise workflows, optimization, and government contract analysis. How may I assist you?"
            },
            "workflow": {
                "name": "AthenaMist Workflow",
                "style": "efficient and organized",
                "focus": "workflow automation and productivity",
                "greeting": "ğŸš€ Welcome! I am AthenaMist Workflow, your AI partner for streamlined productivity and government contract opportunities. Let's optimize your workflow!"
            },
            "government": {
                "name": "AthenaMist Government",
                "style": "official and comprehensive",
                "focus": "government contracts and SAM data analysis",
                "greeting": "ğŸ›ï¸ Greetings! I am AthenaMist Government, your AI specialist for US Government contracts and SAM database insights. How can I help with your government contracting needs?"
            }
        }
        return personalities.get(mode, personalities["creative"])
    
    async def process_query(self, query: str) -> str:
        """
        Process user query with intelligent routing and response generation
        
        This method implements the core query processing logic with intelligent
        routing between AI providers and SAM data analysis.
        
        Args:
            query (str): User query in natural language
            
        Returns:
            str: AI-generated response with context and insights
            
        Processing Features:
        - Intelligent query classification and routing
        - Context-aware response generation
        - Performance monitoring and optimization
        - Error handling and recovery
        - Conversation history management
        
        Query Routing:
        - SAM-related queries routed to government data analysis
        - General queries routed to AI provider integration
        - Command queries handled by system interface
        - Error queries handled with graceful fallback
        """
        # Update performance metrics and tracking
        start_time = time.time()
        self.query_count += 1
        self.system_status['last_query_time'] = start_time
        
        # Add to conversation history with timestamp
        self.conversation_history.append({
            "user": query, 
            "timestamp": start_time,
            "mode": self.mode
        })
        
        # Intelligent query classification and routing
        sam_keywords = [
            'sam', 'government', 'contract', 'opportunity', 'entity', 
            'duns', 'federal', 'agency', 'solicitation', 'bid'
        ]
        query_lower = query.lower()
        
        try:
            if any(keyword in query_lower for keyword in sam_keywords):
                # Route SAM-related queries to government data analysis
                response = await self.sam_ai.process_sam_query(query)
            else:
                # Route general queries to AI provider integration
                response = await self._generate_ai_response(query)
            
            # Calculate and track response time
            response_time = time.time() - start_time
            self.response_times.append(response_time)
            
            # Keep only recent response times for performance tracking
            if len(self.response_times) > 100:
                self.response_times = self.response_times[-100:]
            
            # Add response to conversation history
            self.conversation_history.append({
                "ai": response, 
                "timestamp": time.time(),
                "response_time": response_time,
                "mode": self.mode
            })
            
            # Update system health status
            self.system_status['health_status'] = 'healthy'
            
            return response
            
        except Exception as e:
            # Handle errors gracefully with fallback response
            self.error_count += 1
            self.system_status['health_status'] = 'error'
            
            # Generate fallback response based on mode
            fallback_response = self._generate_fallback_response(query)
            
            # Add error to conversation history
            self.conversation_history.append({
                "ai": fallback_response,
                "timestamp": time.time(),
                "error": str(e),
                "mode": self.mode
            })
            
            return fallback_response
    
    async def _generate_ai_response(self, query: str) -> str:
        """
        Generate AI response using configured provider with context awareness
        
        This method provides intelligent AI response generation with context
        building, error handling, and performance optimization.
        
        Args:
            query (str): User query to process
            
        Returns:
            str: AI-generated response or fallback response
            
        Features:
        - Context building from conversation history
        - AI provider integration with error handling
        - Performance monitoring and optimization
        - Graceful fallback mechanisms
        - Response quality assurance
        """
        try:
            # Build context from recent conversation history
            context = self._build_context()
            
            # Generate response using configured AI provider
            response = await self.ai_manager.generate_response(query, context, self.mode)
            
            # Validate response quality and content
            if not response or len(response.strip()) < 10:
                return self._generate_fallback_response(query)
            
            return response
            
        except Exception as e:
            # Log error and return fallback response
            print(f"AI response generation error: {e}")
            return self._generate_fallback_response(query)
    
    def _build_context(self) -> str:
        """
        Build context from recent conversation history for AI responses
        
        This method creates intelligent context from conversation history
        to improve AI response quality and relevance.
        
        Returns:
            str: Formatted context string for AI processing
            
        Context Features:
        - Recent conversation history extraction
        - Mode-aware context building
        - Timestamp and relevance filtering
        - Context length optimization
        - Conversation flow preservation
        """
        if not self.conversation_history:
            return ""
        
        # Get last 4 exchanges for context (8 entries: 4 user + 4 AI pairs)
        recent_history = self.conversation_history[-8:]
        context_parts = []
        
        for entry in recent_history:
            if "user" in entry:
                context_parts.append(f"User: {entry['user']}")
            elif "ai" in entry:
                # Truncate long AI responses for context efficiency
                ai_response = entry['ai'][:200] + "..." if len(entry['ai']) > 200 else entry['ai']
                context_parts.append(f"Assistant: {ai_response}")
        
        return "\n".join(context_parts)
    
    def _generate_fallback_response(self, query: str) -> str:
        """
        Generate intelligent fallback responses when AI is unavailable
        
        This method provides contextually appropriate fallback responses
        based on the current mode and query content.
        
        Args:
            query (str): User query for context-aware fallback
            
        Returns:
            str: Intelligent fallback response
            
        Fallback Features:
        - Mode-specific response generation
        - Query content analysis and classification
        - Contextually appropriate suggestions
        - Helpful guidance and direction
        - Error recovery and support
        """
        query_lower = query.lower()
        
        # Creative mode fallback responses
        if self.mode == "creative":
            if "lighting" in query_lower:
                return ("âœ¨ For dramatic lighting, try using three-point lighting with a key light at 45Â°, "
                       "fill light at 90Â°, and back light for separation. Consider using warm tones for "
                       "intimate scenes and cool tones for dramatic effects!")
            elif "texture" in query_lower:
                return ("ğŸ¨ Texture creation is an art! Start with high-resolution base textures, "
                       "add procedural noise for realism, and layer multiple textures for depth. "
                       "Remember to consider scale and UV mapping!")
            elif "animation" in query_lower:
                return ("ğŸ¬ Animation is storytelling in motion! Start with strong keyframes, "
                       "use easing curves for natural movement, and remember the 12 principles "
                       "of animation. What story are you telling?")
            else:
                return ("ğŸŒŸ Your creative vision is unique! Let me help you bring it to life. "
                       "What specific aspect of your project would you like to explore?")
        
        # Technical mode fallback responses
        elif self.mode == "technical":
            if "optimize" in query_lower:
                return ("âš™ï¸ Optimization is key! Consider polygon count, texture resolution, "
                       "and render settings. Use LOD (Level of Detail) systems and efficient "
                       "UV mapping for better performance.")
            elif "render" in query_lower:
                return ("ğŸ–¥ï¸ Rendering efficiently requires balance. Use denoising, optimize "
                       "light bounces, and consider GPU rendering for faster results. "
                       "What render engine are you using?")
            elif "script" in query_lower:
                return ("ğŸ“ Scripting can automate repetitive tasks! Python integration "
                       "allows for custom tools and workflows. What specific task would "
                       "you like to automate?")
            else:
                return ("âš™ï¸ Technical precision leads to better results! How can I help "
                       "you optimize your workflow today?")
        
        # Workflow mode fallback responses
        elif self.mode == "workflow":
            if "workflow" in query_lower:
                return ("ğŸš€ Efficient workflows save time and reduce errors! Consider using "
                       "asset libraries, template files, and automated processes. "
                       "What workflow would you like to streamline?")
            elif "organize" in query_lower:
                return ("ğŸ“ Organization is the foundation of productivity! Use consistent "
                       "naming conventions, folder structures, and version control. "
                       "How can I help you organize your project?")
            elif "automate" in query_lower:
                return ("ğŸ¤– Automation frees you to focus on creativity! Scripts, macros, "
                       "and batch processing can handle repetitive tasks. "
                       "What would you like to automate?")
            else:
                return ("ğŸš€ Let's make your workflow more efficient! What process would "
                       "you like to optimize today?")
        
        # Government mode fallback responses
        else:
            if "government" in query_lower or "contract" in query_lower:
                return ("ğŸ›ï¸ I'm connected to the US Government's SAM database! I can help "
                       "you find contract opportunities, search for entities, and analyze "
                       "government contracting data. What would you like to explore?")
            elif "opportunity" in query_lower:
                return ("ğŸ“‹ I can show you recent government contract opportunities! "
                       "Would you like me to search for specific types of contracts or agencies?")
            elif "search" in query_lower:
                return ("ğŸ” I can search the SAM database for companies, organizations, "
                       "or specific entities. What would you like to search for?")
            else:
                return ("ğŸ›ï¸ I'm your AI assistant for government contracting and SAM data "
                       "analysis. How can I help with your government contracting needs?")
    
    def get_workflow_suggestions(self) -> List[str]:
        """
        Get contextually appropriate workflow suggestions based on current mode
        
        This method provides intelligent workflow suggestions tailored to the
        current AI mode and user context.
        
        Returns:
            List[str]: List of workflow suggestions for current mode
            
        Suggestion Features:
        - Mode-specific workflow recommendations
        - Contextually relevant productivity tips
        - Creative and technical guidance
        - Government contracting insights
        - Workflow optimization strategies
        """
        suggestions = {
            "creative": [
                "ğŸ¨ Create a mood board for your project",
                "âœ¨ Experiment with different lighting setups",
                "ğŸ­ Develop character concepts and backstories",
                "ğŸŒ Design environmental storytelling elements",
                "ğŸ¬ Plan camera movements and composition"
            ],
            "technical": [
                "âš™ï¸ Optimize polygon count and topology",
                "ğŸ–¥ï¸ Set up efficient render settings",
                "ğŸ“ Create precise measurements and proportions",
                "ğŸ”§ Develop custom tools and scripts",
                "ğŸ“Š Analyze performance metrics"
            ],
            "workflow": [
                "ğŸ“ Organize project files and assets",
                "ğŸ”„ Create reusable templates and presets",
                "â±ï¸ Set up time tracking and milestones",
                "ğŸ¤– Automate repetitive tasks",
                "ğŸ“‹ Establish quality control checklists"
            ],
            "government": [
                "ğŸ›ï¸ Search for government contract opportunities",
                "ğŸ” Look up companies in the SAM database",
                "ğŸ“Š Analyze government contracting trends",
                "ğŸ“‹ Review recent contract awards",
                "ğŸ¢ Research potential business partners"
            ]
        }
        return suggestions.get(self.mode, suggestions["creative"])
    
    def get_system_status(self) -> Dict:
        """
        Get comprehensive system status and performance metrics
        
        This method provides detailed system health information including
        performance metrics, error rates, and operational status.
        
        Returns:
            Dict: Comprehensive system status information
            
        Status Information:
        - Performance metrics and response times
        - Error rates and health indicators
        - Configuration and mode information
        - Conversation history statistics
        - System uptime and operational status
        """
        avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0
        error_rate = (self.error_count / max(self.query_count, 1)) * 100
        
        return {
            'mode': self.mode,
            'ai_provider': self.system_status['ai_provider'],
            'query_count': self.query_count,
            'error_count': self.error_count,
            'error_rate': f"{error_rate:.1f}%",
            'average_response_time': f"{avg_response_time:.2f}s",
            'uptime': f"{(time.time() - self.start_time) / 3600:.1f}h",
            'health_status': self.system_status['health_status'],
            'conversation_history_length': len(self.conversation_history),
            'last_query_time': self.system_status['last_query_time']
        }

class StandaloneDemo:
    """
    Standalone Demo Application for AthenaMist-Blended
    
    This class provides the main application interface for the standalone demo,
    offering an interactive command-line experience with comprehensive features.
    
    Features:
    - Interactive command-line interface
    - Multi-mode AI personality switching
    - Real-time AI response generation
    - SAM government data integration
    - Comprehensive help system and documentation
    - Performance monitoring and system status
    - Command processing and routing
    
    User Experience:
    - Intuitive command processing
    - Natural language interaction
    - Real-time feedback and responses
    - Comprehensive error handling
    - Performance optimization
    """
    
    def __init__(self):
        """
        Initialize standalone demo application with comprehensive setup
        
        This method sets up the demo application with environment configuration,
        AI provider initialization, and user interface preparation.
        
        Initialization Features:
        - Environment variable configuration
        - AI provider setup and validation
        - User interface initialization
        - Performance monitoring setup
        - Error handling configuration
        """
        # Check for API keys in environment variables
        self.mistral_api_key = os.getenv("MISTRAL_API_KEY", "")
        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")
        
        # Determine AI provider preference
        if self.mistral_api_key:
            self.ai_provider = "mistral"
            self.ai_api_key = self.mistral_api_key
        elif self.openai_api_key:
            self.ai_provider = "openai"
            self.ai_api_key = self.openai_api_key
        else:
            self.ai_provider = "mistral"
            self.ai_api_key = None
        
        # Initialize AI system with default creative mode
        self.ai = AthenaMistAI(mode="creative", ai_provider=self.ai_provider, ai_api_key=self.ai_api_key)
        
        # Application state and configuration
        self.running = True
        self.command_history = []
        self.session_start_time = time.time()
        
    def print_banner(self):
        """
        Display application banner with comprehensive information
        
        This method presents a welcoming banner with system information,
        configuration details, and helpful guidance for users.
        
        Banner Features:
        - Application branding and version information
        - AI provider and mode configuration
        - System status and health indicators
        - Quick start guidance and tips
        - Performance and feature highlights
        """
        print("=" * 80)
        print("ğŸŒŸ AthenaMist-Blended - Advanced AI Integration Framework")
        print("=" * 80)
        print(f"ğŸ¤– AI Provider: {self.ai_provider.title()}")
        print(f"ğŸ­ Current Mode: {self.ai.mode.title()}")
        print(f"ğŸ”‘ API Key: {'âœ… Configured' if self.ai_api_key else 'âŒ Not configured'}")
        print(f"ğŸ›ï¸ SAM Integration: âœ… Active")
        print("=" * 80)
        print("ğŸ’¡ Type '/help' for commands, '/mode <mode>' to switch personalities")
        print("ğŸš€ Ready to assist with creative workflows and government contracts!")
        print("=" * 80)
        print()
    
    def print_help(self):
        """
        Display comprehensive help system with command documentation
        
        This method provides detailed help information including all available
        commands, usage examples, and system features.
        
        Help Features:
        - Complete command reference
        - Usage examples and syntax
        - Feature descriptions and capabilities
        - Configuration and setup guidance
        - Troubleshooting and support information
        """
        print("\n" + "=" * 60)
        print("ğŸ® AthenaMist-Blended Command Reference")
        print("=" * 60)
        
        print("\nğŸŒŸ Core Commands:")
        print("  /help              - Show this help information")
        print("  /mode <mode>       - Switch AI mode (creative/technical/workflow/government)")
        print("  /suggestions       - Get workflow suggestions for current mode")
        print("  /insights          - Show AI insights and performance metrics")
        print("  /history           - Show conversation history")
        print("  /clear             - Clear conversation history")
        
        print("\nğŸ”§ Status Commands:")
        print("  /sam_status        - Check SAM integration status")
        print("  /ai_status         - Check AI integration status")
        print("  /system_status     - Comprehensive system health and metrics")
        
        print("\nâš™ï¸ Configuration Commands:")
        print("  /set_api_key <provider> <key> - Set AI API key")
        print("  /switch_provider <provider>    - Switch AI providers")
        print("  /config            - View current configuration")
        
        print("\nğŸšª Utility Commands:")
        print("  /quit              - Exit AthenaMist gracefully")
        print("  /version           - Show version and build information")
        print("  /debug             - Enable debug mode for troubleshooting")
        
        print("\nğŸ’¡ Usage Examples:")
        print("  'Help me with lighting setup'     - Creative workflow assistance")
        print("  'Optimize my render settings'     - Technical optimization")
        print("  'Search for software companies'   - SAM entity search")
        print("  'Show recent opportunities'       - Government contract data")
        
        print("\nğŸ­ AI Modes:")
        print("  creative    - Artistic and imaginative responses")
        print("  technical   - Precise and analytical responses")
        print("  workflow    - Practical and efficiency-focused")
        print("  government  - SAM and contract-focused responses")
        
        print("=" * 60)
        print()
    
    async def run(self):
        """
        Main application loop with comprehensive user interaction
        
        This method implements the main application loop with command processing,
        user interaction, and system management.
        
        Application Features:
        - Interactive command-line interface
        - Real-time AI response generation
        - Command processing and routing
        - Error handling and recovery
        - Performance monitoring and optimization
        - Session management and cleanup
        """
        # Display welcome banner
        self.print_banner()
        
        # Main application loop
        while self.running:
            try:
                # Get user input with prompt
                user_input = input(f"ğŸŒŸ {self.ai.personality['name']} > ").strip()
                
                # Handle empty input
                if not user_input:
                    continue
                
                # Add to command history
                self.command_history.append({
                    'command': user_input,
                    'timestamp': time.time()
                })
                
                # Process command or query
                if user_input.startswith('/'):
                    await self.handle_command(user_input)
                else:
                    # Process natural language query
                    print("ğŸ¤” Processing your query...")
                    response = await self.ai.process_query(user_input)
                    print(f"\n{response}\n")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye! Thanks for using AthenaMist-Blended!")
                self.running = False
                break
            except Exception as e:
                print(f"\nâŒ Error: {str(e)}")
                print("ğŸ’¡ Try '/help' for assistance or restart the application.")
    
    async def handle_command(self, command: str):
        """
        Process system commands with comprehensive functionality
        
        This method handles all system commands including mode switching,
        status checks, configuration management, and utility functions.
        
        Args:
            command (str): Command string to process
            
        Command Categories:
        - Core functionality commands
        - Status and health monitoring
        - Configuration management
        - Utility and system commands
        - Help and documentation
        """
        parts = command.split()
        cmd = parts[0].lower()
        
        try:
            if cmd == '/help':
                self.print_help()
                
            elif cmd == '/mode':
                if len(parts) > 1:
                    new_mode = parts[1].lower()
                    if new_mode in ['creative', 'technical', 'workflow', 'government']:
                        self.ai = AthenaMistAI(mode=new_mode, ai_provider=self.ai_provider, ai_api_key=self.ai_api_key)
                        print(f"ğŸ­ Switched to {new_mode.title()} mode!")
                        print(f"ğŸ’¬ {self.ai.personality['greeting']}")
                    else:
                        print("âŒ Invalid mode. Use: creative, technical, workflow, or government")
                else:
                    print(f"ğŸ­ Current mode: {self.ai.mode.title()}")
                    print("ğŸ’¡ Use '/mode <mode>' to switch (creative/technical/workflow/government)")
            
            elif cmd == '/suggestions':
                suggestions = self.ai.get_workflow_suggestions()
                print(f"\nğŸ’¡ Workflow Suggestions for {self.ai.mode.title()} Mode:")
                for i, suggestion in enumerate(suggestions, 1):
                    print(f"  {i}. {suggestion}")
                print()
            
            elif cmd == '/insights':
                status = self.ai.get_system_status()
                print(f"\nğŸ“Š AI Insights and Performance Metrics:")
                print(f"  Mode: {status['mode'].title()}")
                print(f"  AI Provider: {status['ai_provider'].title()}")
                print(f"  Queries Processed: {status['query_count']}")
                print(f"  Error Rate: {status['error_rate']}")
                print(f"  Avg Response Time: {status['average_response_time']}")
                print(f"  System Health: {status['health_status']}")
                print(f"  Conversation History: {status['conversation_history_length']} entries")
                print()
            
            elif cmd == '/history':
                if self.ai.conversation_history:
                    print(f"\nğŸ“œ Recent Conversation History:")
                    for i, entry in enumerate(self.ai.conversation_history[-10:], 1):
                        if 'user' in entry:
                            print(f"  {i}. You: {entry['user'][:100]}...")
                        elif 'ai' in entry:
                            print(f"     AI: {entry['ai'][:100]}...")
                    print()
                else:
                    print("ğŸ“œ No conversation history yet.")
            
            elif cmd == '/clear':
                self.ai.conversation_history = []
                print("ğŸ§¹ Conversation history cleared!")
            
            elif cmd == '/sam_status':
                status = self.ai.sam_integration.get_integration_status()
                print(f"\nğŸ›ï¸ SAM Integration Status:")
                print(f"  API Key Configured: {'âœ… Yes' if status['api_key_configured'] else 'âŒ No'}")
                print(f"  API Key Valid: {'âœ… Yes' if status['api_key_valid'] else 'âŒ No'}")
                print(f"  Session Active: {'âœ… Yes' if status['session_active'] else 'âŒ No'}")
                print(f"  Cache Enabled: {'âœ… Yes' if status['cache_enabled'] else 'âŒ No'}")
                print()
            
            elif cmd == '/ai_status':
                ai_status = self.ai.ai_manager.get_status()
                print(f"\nğŸ¤– AI Integration Status:")
                print(f"  Provider: {ai_status['provider'].title()}")
                print(f"  API Key Configured: {'âœ… Yes' if ai_status['api_key_configured'] else 'âŒ No'}")
                print(f"  Total Requests: {ai_status['performance_metrics']['total_requests']}")
                print(f"  Success Rate: {ai_status['performance_metrics']['successful_requests'] / max(ai_status['performance_metrics']['total_requests'], 1) * 100:.1f}%")
                print()
            
            elif cmd == '/system_status':
                status = self.ai.get_system_status()
                print(f"\nğŸ”§ Comprehensive System Status:")
                print(f"  Application Uptime: {status['uptime']}")
                print(f"  AI Mode: {status['mode'].title()}")
                print(f"  AI Provider: {status['ai_provider'].title()}")
                print(f"  Health Status: {status['health_status']}")
                print(f"  Performance Metrics:")
                print(f"    - Queries: {status['query_count']}")
                print(f"    - Error Rate: {status['error_rate']}")
                print(f"    - Avg Response: {status['average_response_time']}")
                print(f"    - History: {status['conversation_history_length']} entries")
                print()
            
            elif cmd == '/set_api_key':
                if len(parts) >= 3:
                    provider = parts[1].lower()
                    api_key = parts[2]
                    if provider in ['mistral', 'openai']:
                        self.ai.ai_manager.update_api_key(api_key)
                        self.ai_api_key = api_key
                        print(f"âœ… {provider.title()} API key updated!")
                    else:
                        print("âŒ Invalid provider. Use 'mistral' or 'openai'")
                else:
                    print("âŒ Usage: /set_api_key <provider> <key>")
            
            elif cmd == '/switch_provider':
                if len(parts) > 1:
                    provider = parts[1].lower()
                    if provider in ['mistral', 'openai']:
                        self.ai.ai_manager.switch_provider(provider, self.ai_api_key)
                        self.ai_provider = provider
                        print(f"ğŸ”„ Switched to {provider.title()} provider!")
                    else:
                        print("âŒ Invalid provider. Use 'mistral' or 'openai'")
                else:
                    print("âŒ Usage: /switch_provider <provider>")
            
            elif cmd == '/config':
                print(f"\nâš™ï¸ Current Configuration:")
                print(f"  AI Provider: {self.ai_provider.title()}")
                print(f"  AI Mode: {self.ai.mode.title()}")
                print(f"  API Key: {'âœ… Configured' if self.ai_api_key else 'âŒ Not configured'}")
                print(f"  SAM Integration: âœ… Active")
                print()
            
            elif cmd == '/quit':
                print("\nğŸ‘‹ Thank you for using AthenaMist-Blended!")
                print("ğŸŒŸ May your creative workflows be inspired and your government contracts be successful!")
                self.running = False
            
            elif cmd == '/version':
                print(f"\nğŸ“‹ AthenaMist-Blended Version Information:")
                print(f"  Version: 1.0.0")
                print(f"  Architecture: Modular AI Integration Framework")
                print(f"  Python: 3.8+")
                print(f"  Features: Multi-Provider AI, SAM Integration, Async Operations")
                print()
            
            elif cmd == '/debug':
                print(f"\nğŸ› Debug Information:")
                print(f"  Session Start: {time.ctime(self.session_start_time)}")
                print(f"  Command History: {len(self.command_history)} commands")
                print(f"  AI Status: {self.ai.get_system_status()}")
                print(f"  SAM Status: {self.ai.sam_integration.get_integration_status()}")
                print()
            
            else:
                print(f"âŒ Unknown command: {cmd}")
                print("ğŸ’¡ Type '/help' for available commands")
                
        except Exception as e:
            print(f"âŒ Command error: {str(e)}")
            print("ğŸ’¡ Try '/help' for assistance")

async def main():
    """
    Main application entry point with comprehensive initialization
    
    This function serves as the main entry point for the AthenaMist-Blended
    standalone demo application.
    
    Features:
    - Application initialization and setup
    - Error handling and recovery
    - Performance monitoring and optimization
    - Graceful shutdown and cleanup
    - System health and status management
    """
    try:
        # Initialize and run the standalone demo
        demo = StandaloneDemo()
        await demo.run()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Application interrupted. Goodbye!")
    except Exception as e:
        print(f"\nâŒ Application error: {str(e)}")
        print("ğŸ’¡ Please check your configuration and try again.")

if __name__ == "__main__":
    # Run the main application with async support
    asyncio.run(main()) 